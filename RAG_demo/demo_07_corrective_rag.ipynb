{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e4ab30",
   "metadata": {},
   "source": [
    "# Demo #7: Corrective RAG (Self-Reflective Retrieval)\n",
    "\n",
    "## ğŸ¯ Learning Objectives\n",
    "\n",
    "In this demonstration, you will learn:\n",
    "\n",
    "1. **Retrieval Quality Evaluation**: How to assess the relevance of retrieved documents using LLM-based grading\n",
    "2. **Self-Corrective Mechanisms**: How to implement conditional workflows based on retrieval assessment\n",
    "3. **Fallback Strategies**: How to trigger alternative paths (re-retrieval, web search, direct generation)\n",
    "4. **Adaptive Behavior**: How to build RAG systems that \"reflect\" on their own performance\n",
    "\n",
    "## ğŸ“š Theoretical Background\n",
    "\n",
    "### The Problem with Static RAG Pipelines\n",
    "\n",
    "Traditional RAG systems follow a fixed pipeline: **Retrieve â†’ Generate**. They assume that:\n",
    "- The retrieved documents are always relevant\n",
    "- The knowledge base contains the answer\n",
    "- The retrieval method successfully found the right content\n",
    "\n",
    "However, these assumptions often fail in practice, leading to common failure modes:\n",
    "\n",
    "#### Common RAG Failure Points (from the curriculum)\n",
    "\n",
    "- **FP1: Missing Content (Corpus Failure)**: The knowledge base doesn't contain the answer â†’ System may hallucinate\n",
    "- **FP2: Missed Top Ranked (Retrieval Failure)**: The correct document exists but isn't retrieved â†’ Poor answers despite good data\n",
    "- **FP3: Not in Context (Post-Retrieval Failure)**: Document was retrieved but filtered out â†’ Information loss\n",
    "- **FP4: Not Extracted (Generation Failure)**: Answer is in context but LLM fails to extract it â†’ Wasted retrieval effort\n",
    "\n",
    "### The Corrective RAG Solution\n",
    "\n",
    "Corrective RAG (CRAG) introduces a **self-reflective layer** between retrieval and generation:\n",
    "\n",
    "```\n",
    "Traditional RAG:  Query â†’ Retrieve â†’ Generate\n",
    "\n",
    "Corrective RAG:   Query â†’ Retrieve â†’ EVALUATE â†’ [Route Decision]\n",
    "                                        â†“\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â†“               â†“               â†“\n",
    "                   RELEVANT      PARTIALLY         NOT RELEVANT\n",
    "                        â†“           RELEVANT            â†“\n",
    "                   Generate    Re-retrieve +       Web Search\n",
    "                               Transform            Fallback\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "1. **Relevance Grader**: An LLM-based critic that scores retrieval quality (1-5 scale)\n",
    "2. **Decision Logic**: Conditional routing based on relevance scores\n",
    "3. **Corrective Actions**:\n",
    "   - **High Relevance (4-5)**: Proceed with standard generation\n",
    "   - **Medium Relevance (2-3)**: Trigger query transformation and re-retrieval\n",
    "   - **Low Relevance (<2)**: Fall back to web search or direct LLM knowledge\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Corrective RAG transforms the system from a **blind executor** into a **reflective agent** that:\n",
    "- **Detects its own failures** before producing bad answers\n",
    "- **Takes corrective action** to improve retrieval quality\n",
    "- **Falls back gracefully** when knowledge is missing\n",
    "- **Reduces hallucinations** by recognizing insufficient context\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”§ Implementation\n",
    "\n",
    "We'll implement a complete Corrective RAG system with:\n",
    "- A deliberately limited corpus (to trigger failure scenarios)\n",
    "- An LLM-based relevance grader\n",
    "- Three retrieval paths (standard, corrective, fallback)\n",
    "- Test queries designed to exercise each path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a20dd560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.14.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "  Downloading llama_index_core-0.14.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-index-llms-azure-openai\n",
      "Collecting llama-index-llms-azure-openai\n",
      "  Downloading llama_index_llms_azure_openai-0.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading llama_index_llms_azure_openai-0.4.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting llama-index-embeddings-azure-openai\n",
      "Collecting llama-index-embeddings-azure-openai\n",
      "  Downloading llama_index_embeddings_azure_openai-0.4.1-py3-none-any.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: python-dotenv in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (1.1.0)\n",
      "  Downloading llama_index_embeddings_azure_openai-0.4.1-py3-none-any.whl.metadata (503 bytes)\n",
      "Requirement already satisfied: python-dotenv in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (1.1.0)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (3.11.18)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (3.11.18)\n",
      "Collecting aiosqlite (from llama-index-core)\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting aiosqlite (from llama-index-core)\n",
      "  Using cached aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting banks<3,>=2.2.0 (from llama-index-core)\n",
      "  Downloading banks-2.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: dataclasses-json in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (1.2.18)\n",
      "Requirement already satisfied: dataclasses-json in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (1.2.18)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting dirtyjson<2,>=1.0.8 (from llama-index-core)\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2025.3.2)\n",
      "Requirement already satisfied: httpx in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.28.1)\n",
      "Collecting filetype<2,>=1.2.0 (from llama-index-core)\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2025.3.2)\n",
      "Requirement already satisfied: httpx in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.28.1)\n",
      "Collecting llama-index-workflows<3,>=2 (from llama-index-core)\n",
      "Collecting llama-index-workflows<3,>=2 (from llama-index-core)\n",
      "  Downloading llama_index_workflows-2.8.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (3.4.2)\n",
      "  Downloading llama_index_workflows-2.8.1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (3.4.2)\n",
      "Collecting nltk>3.8.1 (from llama-index-core)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2.3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (11.2.1)\n",
      "Requirement already satisfied: platformdirs in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2.32.5)\n",
      "Collecting nltk>3.8.1 (from llama-index-core)\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2.3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (11.2.1)\n",
      "Requirement already satisfied: platformdirs in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (4.3.8)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2.11.7)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.31.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (2.32.5)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (9.1.2)\n",
      "Collecting setuptools>=80.9.0 (from llama-index-core)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core) (2.0.43)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (9.1.2)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.20.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (4.13.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from llama-index-core) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from aiohttp<4,>=3.8.6->llama-index-core) (1.20.0)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core)\n",
      "Collecting griffe (from banks<3,>=2.2.0->llama-index-core)\n",
      "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "  Downloading griffe-1.14.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core) (3.1.6)\n",
      "Requirement already satisfied: jinja2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from banks<3,>=2.2.0->llama-index-core) (3.1.6)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<3,>=2->llama-index-core)\n",
      "Collecting llama-index-instrumentation>=0.1.0 (from llama-index-workflows<3,>=2->llama-index-core)\n",
      "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core) (3.7)\n",
      "Requirement already satisfied: idna>=2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp<4,>=3.8.6->llama-index-core) (3.7)\n",
      "Collecting azure-identity<2,>=1.15.0 (from llama-index-llms-azure-openai)\n",
      "Collecting azure-identity<2,>=1.15.0 (from llama-index-llms-azure-openai)\n",
      "  Downloading azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
      "  Downloading azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
      "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama-index-llms-azure-openai)\n",
      "Collecting llama-index-llms-openai<0.7,>=0.6.0 (from llama-index-llms-azure-openai)\n",
      "  Downloading llama_index_llms_openai-0.6.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "  Downloading llama_index_llms_openai-0.6.5-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n",
      "Collecting azure-core>=1.31.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (44.0.3)\n",
      "Requirement already satisfied: cryptography>=2.5 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (44.0.3)\n",
      "Collecting msal>=1.30.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity<2,>=1.15.0->llama-index-llms-azure-openai)\n",
      "  Using cached msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting openai<2,>=1.108.1 (from llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting openai<2,>=1.108.1 (from llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai)\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from openai<2,>=1.108.1->llama-index-llms-openai<0.7,>=0.6.0->llama-index-llms-azure-openai) (1.3.1)\n",
      "Requirement already satisfied: certifi in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from httpx->llama-index-core) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from httpx->llama-index-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.16.0)\n",
      "Requirement already satisfied: certifi in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from httpx->llama-index-core) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from httpx->llama-index-core) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (0.4.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (0.4.0)\n",
      "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index-embeddings-azure-openai)\n",
      "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
      "Collecting llama-index-embeddings-openai<0.6,>=0.5.0 (from llama-index-embeddings-azure-openai)\n",
      "  Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl.metadata (400 bytes)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (2.22)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (2.22)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (2.10.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (2.4.0)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2,>=1.15.0->llama-index-llms-azure-openai) (2.10.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (2.4.0)\n",
      "Requirement already satisfied: click in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: greenlet>=1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core) (3.2.4)\n",
      "Requirement already satisfied: click in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (8.1.8)\n",
      "Requirement already satisfied: joblib in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: greenlet>=1 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.1.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from dataclasses-json->llama-index-core) (3.26.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (24.2)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core) (3.0.2)\n",
      "Collecting colorama>=0.4 (from griffe->banks<3,>=2.2.0->llama-index-core)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/alibina/anaconda3/envs/crewai/lib/python3.11/site-packages (from jinja2->banks<3,>=2.2.0->llama-index-core) (3.0.2)\n",
      "Downloading llama_index_core-0.14.5-py3-none-any.whl (11.9 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/11.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading llama_index_core-0.14.5-py3-none-any.whl (11.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading banks-2.2.0-py3-none-any.whl (29 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading llama_index_workflows-2.8.1-py3-none-any.whl (61 kB)\n",
      "Downloading llama_index_workflows-2.8.1-py3-none-any.whl (61 kB)\n",
      "Downloading llama_index_llms_azure_openai-0.4.2-py3-none-any.whl (7.3 kB)\n",
      "Downloading llama_index_llms_azure_openai-0.4.2-py3-none-any.whl (7.3 kB)\n",
      "Downloading azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
      "Downloading azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
      "Downloading llama_index_llms_openai-0.6.5-py3-none-any.whl (26 kB)\n",
      "Downloading llama_index_llms_openai-0.6.5-py3-none-any.whl (26 kB)\n",
      "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/948.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading llama_index_embeddings_azure_openai-0.4.1-py3-none-any.whl (4.4 kB)\n",
      "Downloading llama_index_embeddings_azure_openai-0.4.1-py3-none-any.whl (4.4 kB)\n",
      "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading azure_core-1.36.0-py3-none-any.whl (213 kB)\n",
      "Downloading llama_index_embeddings_openai-0.5.1-py3-none-any.whl (7.0 kB)\n",
      "Downloading azure_core-1.36.0-py3-none-any.whl (213 kB)\n",
      "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Downloading llama_index_instrumentation-0.4.2-py3-none-any.whl (15 kB)\n",
      "Downloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "Downloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
      "Using cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mUsing cached msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
      "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading griffe-1.14.0-py3-none-any.whl (144 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Installing collected packages: filetype, dirtyjson, setuptools, nltk, colorama, aiosqlite, griffe, azure-core, openai, llama-index-instrumentation, banks, msal, llama-index-workflows, msal-extensions, llama-index-core, llama-index-llms-openai, llama-index-embeddings-openai, azure-identity, llama-index-llms-azure-openai, llama-index-embeddings-azure-openai\n",
      "\u001b[?25lInstalling collected packages: filetype, dirtyjson, setuptools, nltk, colorama, aiosqlite, griffe, azure-core, openai, llama-index-instrumentation, banks, msal, llama-index-workflows, msal-extensions, llama-index-core, llama-index-llms-openai, llama-index-embeddings-openai, azure-identity, llama-index-llms-azure-openai, llama-index-embeddings-azure-openai\n",
      "\u001b[2K  Attempting uninstall: setuptools\n",
      "\u001b[2K    Found existing installation: setuptools 78.1.1\n",
      "\u001b[2K  Attempting uninstall: setuptools\n",
      "\u001b[2K    Found existing installation: setuptools 78.1.1\n",
      "\u001b[2K    Uninstalling setuptools-78.1.1:\n",
      "\u001b[2K    Uninstalling setuptools-78.1.1:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/20\u001b[0m [setuptools]\n",
      "\u001b[2K      Successfully uninstalled setuptools-78.1.1â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 2/20\u001b[0m [setuptools]\n",
      "\u001b[2K  Attempting uninstall: openaimâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/20\u001b[0m [azure-core]\n",
      "\u001b[2K    Found existing installation: openai 1.75.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 7/20\u001b[0m [azure-core]\n",
      "\u001b[2K    Uninstalling openai-1.75.0:0mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/20\u001b[0m [openai]\n",
      "\u001b[2K      Successfully uninstalled openai-1.75.0â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 8/20\u001b[0m [openai]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m20/20\u001b[0m [llama-index-embeddings-azure-openai]ings-azure-openai]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dspy 3.0.1 requires gepa[dspy]==0.0.4, which is not installed.\n",
      "crewai 0.120.1 requires litellm==1.68.0, but you have litellm 1.45.0 which is incompatible.\n",
      "mem0ai 0.1.98 requires pytz<2025.0,>=2024.1, but you have pytz 2025.2 which is incompatible.\n",
      "dspy 3.0.1 requires litellm>=1.64.0, but you have litellm 1.45.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed aiosqlite-0.21.0 azure-core-1.36.0 azure-identity-1.25.1 banks-2.2.0 colorama-0.4.6 dirtyjson-1.0.8 filetype-1.2.0 griffe-1.14.0 llama-index-core-0.14.5 llama-index-embeddings-azure-openai-0.4.1 llama-index-embeddings-openai-0.5.1 llama-index-instrumentation-0.4.2 llama-index-llms-azure-openai-0.4.2 llama-index-llms-openai-0.6.5 llama-index-workflows-2.8.1 msal-1.34.0 msal-extensions-1.3.1 nltk-3.9.2 openai-1.109.1 setuptools-80.9.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install llama-index-core llama-index-llms-azure-openai llama-index-embeddings-azure-openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1e5f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.llms.azure_openai import AzureOpenAI\n",
    "from llama_index.embeddings.azure_openai import AzureOpenAIEmbedding\n",
    "from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.core.schema import QueryBundle\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e585b61",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "Configure Azure OpenAI for both LLM (GPT-4) and embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e677e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Azure OpenAI configured successfully\n",
      "   LLM: gpt-4\n",
      "   Embeddings: text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "# Configure Azure OpenAI LLM\n",
    "llm = AzureOpenAI(\n",
    "    model=\"gpt-4\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    "    temperature=0.1  # Low temperature for consistent evaluation\n",
    ")\n",
    "\n",
    "# Configure Azure OpenAI Embeddings\n",
    "embed_model = AzureOpenAIEmbedding(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"),\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=\"2024-02-15-preview\",\n",
    ")\n",
    "\n",
    "# Set global defaults\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"âœ… Azure OpenAI configured successfully\")\n",
    "print(f\"   LLM: {llm.model}\")\n",
    "print(f\"   Embeddings: {embed_model.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f260b58",
   "metadata": {},
   "source": [
    "## Step 2: Deliberately Limited Data Preparation\n",
    "\n",
    "We'll load only 3 documents from the `tech_docs` directory to create a limited knowledge base. This will enable us to test scenarios where:\n",
    "- **Query is well-covered**: Documents contain comprehensive information\n",
    "- **Query is partially covered**: Documents have some relevant info but gaps\n",
    "- **Query is not covered**: Information is completely missing (FP1: Missing Content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31aa12c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 3 documents\n",
      "\n",
      "ğŸ“„ Document topics:\n",
      "   1. transformer_architecture.md (4247 chars)\n",
      "   2. bert_model.md (2358 chars)\n",
      "   3. gpt4_model.md (2814 chars)\n",
      "\n",
      "âš ï¸  Note: Limited corpus intentionally excludes topics like:\n",
      "   - Docker, REST APIs, embeddings (in tech_docs but not loaded)\n",
      "   - Finance, ML algorithms (in other directories)\n",
      "   - Recent events (not in any document)\n"
     ]
    }
   ],
   "source": [
    "# Load a limited set of documents (3 docs only)\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_files=[\n",
    "        \"data/tech_docs/transformer_architecture.md\",\n",
    "        \"data/tech_docs/bert_model.md\",\n",
    "        \"data/tech_docs/gpt4_model.md\"\n",
    "    ]\n",
    ").load_data()\n",
    "\n",
    "print(f\"âœ… Loaded {len(documents)} documents\")\n",
    "print(\"\\nğŸ“„ Document topics:\")\n",
    "for i, doc in enumerate(documents, 1):\n",
    "    print(f\"   {i}. {doc.metadata.get('file_name', 'Unknown')} ({len(doc.text)} chars)\")\n",
    "\n",
    "print(\"\\nâš ï¸  Note: Limited corpus intentionally excludes topics like:\")\n",
    "print(\"   - Docker, REST APIs, embeddings (in tech_docs but not loaded)\")\n",
    "print(\"   - Finance, ML algorithms (in other directories)\")\n",
    "print(\"   - Recent events (not in any document)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3687726",
   "metadata": {},
   "source": [
    "## Step 3: Build Baseline Vector Index\n",
    "\n",
    "Create a standard vector index for retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58609da6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Vector index built successfully\n",
      "   Configured retriever with top_k=3\n"
     ]
    }
   ],
   "source": [
    "# Build vector index\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "print(\"âœ… Vector index built successfully\")\n",
    "\n",
    "# Create retriever with moderate top_k\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "print(f\"   Configured retriever with top_k=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154371d8",
   "metadata": {},
   "source": [
    "## Step 4: Implement Relevance Grader\n",
    "\n",
    "This is the **core component** of Corrective RAG. The grader:\n",
    "1. Takes a query and retrieved documents\n",
    "2. Uses an LLM to assess relevance on a 1-5 scale\n",
    "3. Returns both a score and a decision (RELEVANT/PARTIAL/NOT_RELEVANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98dd802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Relevance grader initialized\n"
     ]
    }
   ],
   "source": [
    "class RelevanceDecision(Enum):\n",
    "    \"\"\"Enum for relevance decision outcomes\"\"\"\n",
    "    RELEVANT = \"relevant\"  # Score 4-5: Proceed with generation\n",
    "    PARTIAL = \"partial\"    # Score 2-3: Re-retrieve with query transformation\n",
    "    NOT_RELEVANT = \"not_relevant\"  # Score <2: Fallback to web search or direct LLM\n",
    "\n",
    "\n",
    "class RelevanceGrader:\n",
    "    \"\"\"LLM-based relevance grader for assessing retrieval quality\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        \n",
    "    def assess_relevance(\n",
    "        self, \n",
    "        query: str, \n",
    "        retrieved_texts: List[str]\n",
    "    ) -> Tuple[float, RelevanceDecision, str]:\n",
    "        \"\"\"\n",
    "        Assess the relevance of retrieved documents to the query.\n",
    "        \n",
    "        Args:\n",
    "            query: The user's query\n",
    "            retrieved_texts: List of retrieved document texts\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (relevance_score, decision, explanation)\n",
    "        \"\"\"\n",
    "        # Combine retrieved texts\n",
    "        combined_context = \"\\n\\n---\\n\\n\".join(\n",
    "            [f\"Document {i+1}:\\n{text[:500]}...\" \n",
    "             for i, text in enumerate(retrieved_texts)]\n",
    "        )\n",
    "        \n",
    "        # Construct grading prompt\n",
    "        grading_prompt = f\"\"\"You are a relevance grading expert. Your task is to assess how relevant a set of retrieved documents are to answering a user's query.\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "RETRIEVED DOCUMENTS:\n",
    "{combined_context}\n",
    "\n",
    "GRADING INSTRUCTIONS:\n",
    "Assess the relevance of these documents on a 1-5 scale:\n",
    "- 5: Highly relevant - Documents directly and comprehensively address the query\n",
    "- 4: Relevant - Documents contain substantial information to answer the query\n",
    "- 3: Partially relevant - Documents have some related information but with gaps\n",
    "- 2: Minimally relevant - Documents tangentially related but insufficient\n",
    "- 1: Not relevant - Documents do not contain information to answer the query\n",
    "\n",
    "Respond ONLY in this exact format:\n",
    "SCORE: [number 1-5]\n",
    "REASONING: [One sentence explaining your score]\n",
    "\"\"\"\n",
    "        \n",
    "        # Get LLM assessment\n",
    "        response = self.llm.complete(grading_prompt)\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        # Parse response\n",
    "        try:\n",
    "            score_line = [l for l in response_text.split('\\n') if l.startswith('SCORE:')][0]\n",
    "            score = float(score_line.split('SCORE:')[1].strip())\n",
    "            \n",
    "            reasoning_line = [l for l in response_text.split('\\n') if l.startswith('REASONING:')][0]\n",
    "            reasoning = reasoning_line.split('REASONING:')[1].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Failed to parse LLM response: {e}\")\n",
    "            print(f\"   Response: {response_text}\")\n",
    "            score = 3.0  # Default to partial\n",
    "            reasoning = \"Failed to parse grading response\"\n",
    "        \n",
    "        # Make decision based on score\n",
    "        if score >= 4.0:\n",
    "            decision = RelevanceDecision.RELEVANT\n",
    "        elif score >= 2.0:\n",
    "            decision = RelevanceDecision.PARTIAL\n",
    "        else:\n",
    "            decision = RelevanceDecision.NOT_RELEVANT\n",
    "        \n",
    "        return score, decision, reasoning\n",
    "\n",
    "\n",
    "# Initialize grader\n",
    "grader = RelevanceGrader(llm)\n",
    "print(\"âœ… Relevance grader initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b31abb",
   "metadata": {},
   "source": [
    "## Step 5: Implement Corrective RAG System\n",
    "\n",
    "Now we'll build the complete Corrective RAG pipeline with three execution paths:\n",
    "\n",
    "1. **Path A (Relevant)**: Standard generation with retrieved context\n",
    "2. **Path B (Partial)**: Re-retrieval with HyDE query transformation\n",
    "3. **Path C (Not Relevant)**: Fallback to direct LLM generation (simulating web search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bea23f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Corrective RAG system initialized\n"
     ]
    }
   ],
   "source": [
    "class CorrectiveRAG:\n",
    "    \"\"\"Corrective RAG system with self-reflective retrieval\"\"\"\n",
    "    \n",
    "    def __init__(self, index, llm, grader):\n",
    "        self.index = index\n",
    "        self.llm = llm\n",
    "        self.grader = grader\n",
    "        self.retriever = index.as_retriever(similarity_top_k=3)\n",
    "        self.hyde_transform = HyDEQueryTransform(llm=llm, include_original=True)\n",
    "        \n",
    "    def query(\n",
    "        self, \n",
    "        query_text: str, \n",
    "        verbose: bool = True\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute corrective RAG query with adaptive routing.\n",
    "        \n",
    "        Returns dict with:\n",
    "            - answer: Final generated answer\n",
    "            - path: Execution path taken\n",
    "            - relevance_score: Initial relevance assessment\n",
    "            - decision: Routing decision\n",
    "            - metadata: Additional execution metadata\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ğŸ” QUERY: {query_text}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Step 1: Initial Retrieval\n",
    "        if verbose:\n",
    "            print(\"ğŸ“¥ STEP 1: Initial Retrieval\")\n",
    "        \n",
    "        retrieved_nodes = self.retriever.retrieve(query_text)\n",
    "        retrieved_texts = [node.node.text for node in retrieved_nodes]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Retrieved {len(retrieved_texts)} documents\")\n",
    "            for i, text in enumerate(retrieved_texts, 1):\n",
    "                print(f\"   Doc {i}: {text[:100]}...\")\n",
    "        \n",
    "        # Step 2: Relevance Assessment\n",
    "        if verbose:\n",
    "            print(\"\\nğŸ§  STEP 2: Relevance Assessment\")\n",
    "        \n",
    "        score, decision, reasoning = self.grader.assess_relevance(\n",
    "            query_text, \n",
    "            retrieved_texts\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"   Relevance Score: {score}/5\")\n",
    "            print(f\"   Decision: {decision.value.upper()}\")\n",
    "            print(f\"   Reasoning: {reasoning}\")\n",
    "        \n",
    "        # Step 3: Conditional Routing\n",
    "        if decision == RelevanceDecision.RELEVANT:\n",
    "            return self._path_relevant(\n",
    "                query_text, \n",
    "                retrieved_nodes, \n",
    "                score, \n",
    "                reasoning, \n",
    "                verbose\n",
    "            )\n",
    "        elif decision == RelevanceDecision.PARTIAL:\n",
    "            return self._path_partial(\n",
    "                query_text, \n",
    "                score, \n",
    "                reasoning, \n",
    "                verbose\n",
    "            )\n",
    "        else:  # NOT_RELEVANT\n",
    "            return self._path_not_relevant(\n",
    "                query_text, \n",
    "                score, \n",
    "                reasoning, \n",
    "                verbose\n",
    "            )\n",
    "    \n",
    "    def _path_relevant(\n",
    "        self, \n",
    "        query_text: str, \n",
    "        retrieved_nodes, \n",
    "        score: float, \n",
    "        reasoning: str, \n",
    "        verbose: bool\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Path A: Standard generation with relevant context\"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\nâœ… STEP 3: Path A - Standard Generation\")\n",
    "            print(\"   Confidence: HIGH - Proceeding with retrieved context\")\n",
    "        \n",
    "        # Use query engine for generation\n",
    "        query_engine = self.index.as_query_engine(similarity_top_k=3)\n",
    "        response = query_engine.query(query_text)\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.response,\n",
    "            \"path\": \"A: RELEVANT\",\n",
    "            \"relevance_score\": score,\n",
    "            \"decision\": \"relevant\",\n",
    "            \"reasoning\": reasoning,\n",
    "            \"source_nodes\": response.source_nodes,\n",
    "            \"corrective_action\": None\n",
    "        }\n",
    "    \n",
    "    def _path_partial(\n",
    "        self, \n",
    "        query_text: str, \n",
    "        score: float, \n",
    "        reasoning: str, \n",
    "        verbose: bool\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Path B: Re-retrieval with HyDE query transformation\"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\nâš ï¸  STEP 3: Path B - Corrective Re-Retrieval\")\n",
    "            print(\"   Confidence: MEDIUM - Attempting query transformation + re-retrieval\")\n",
    "        \n",
    "        # Transform query using HyDE\n",
    "        query_bundle = QueryBundle(query_str=query_text)\n",
    "        transformed_query = self.hyde_transform(query_bundle)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n   ğŸ”„ HyDE Transformation:\")\n",
    "            print(f\"   Original: {query_text}\")\n",
    "            if hasattr(transformed_query, 'embedding_strs'):\n",
    "                print(f\"   Hypothetical Doc: {transformed_query.embedding_strs[0][:200]}...\")\n",
    "        \n",
    "        # Create query engine with HyDE\n",
    "        hyde_query_engine = self.index.as_query_engine(\n",
    "            similarity_top_k=3,\n",
    "            query_transform=self.hyde_transform\n",
    "        )\n",
    "        response = hyde_query_engine.query(query_text)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n   âœ“ Re-retrieved with transformed query\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.response,\n",
    "            \"path\": \"B: PARTIAL (Re-retrieval)\",\n",
    "            \"relevance_score\": score,\n",
    "            \"decision\": \"partial\",\n",
    "            \"reasoning\": reasoning,\n",
    "            \"source_nodes\": response.source_nodes,\n",
    "            \"corrective_action\": \"HyDE query transformation\"\n",
    "        }\n",
    "    \n",
    "    def _path_not_relevant(\n",
    "        self, \n",
    "        query_text: str, \n",
    "        score: float, \n",
    "        reasoning: str, \n",
    "        verbose: bool\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Path C: Fallback to direct LLM generation (simulating web search)\"\"\"\n",
    "        if verbose:\n",
    "            print(\"\\nâŒ STEP 3: Path C - Fallback to Direct LLM\")\n",
    "            print(\"   Confidence: LOW - Knowledge base lacks relevant information\")\n",
    "            print(\"   Action: Using LLM's parametric knowledge (simulating web search)\")\n",
    "        \n",
    "        # Generate directly from LLM without RAG context\n",
    "        fallback_prompt = f\"\"\"The knowledge base does not contain relevant information for this query.\n",
    "Using your parametric knowledge, provide a helpful answer. If you don't know, say so.\n",
    "\n",
    "Query: {query_text}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = self.llm.complete(fallback_prompt)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"   âš ï¸  Note: This is a graceful fallback. In production, this would:\")\n",
    "            print(\"      - Trigger web search (Bing, Google)\")\n",
    "            print(\"      - Query external APIs\")\n",
    "            print(\"      - Or return 'Information not available' with suggestions\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.text,\n",
    "            \"path\": \"C: NOT RELEVANT (Fallback)\",\n",
    "            \"relevance_score\": score,\n",
    "            \"decision\": \"not_relevant\",\n",
    "            \"reasoning\": reasoning,\n",
    "            \"source_nodes\": [],\n",
    "            \"corrective_action\": \"Direct LLM generation (web search simulation)\"\n",
    "        }\n",
    "\n",
    "\n",
    "# Initialize Corrective RAG system\n",
    "crag_system = CorrectiveRAG(index, llm, grader)\n",
    "print(\"âœ… Corrective RAG system initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eaeccd",
   "metadata": {},
   "source": [
    "## Step 6: Test Suite - Exercising All Three Paths\n",
    "\n",
    "We'll design three test queries to exercise each execution path:\n",
    "\n",
    "### Test Query 1: Well-Covered Topic (Expected: Path A - RELEVANT)\n",
    "Query about transformers - topic directly covered in loaded documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "430a4515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” QUERY: What is the self-attention mechanism in transformer architecture and why is it important?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¥ STEP 1: Initial Retrieval\n",
      "   Retrieved 3 documents\n",
      "   Doc 1: # Transformer Architecture in Deep Learning\n",
      "\n",
      "The **Transformer architecture** revolutionized natural...\n",
      "   Doc 2: Multi-head self-attention\n",
      "  2. Layer normalization\n",
      "  3. Feed-forward network\n",
      "  4. Residual connectio...\n",
      "   Doc 3: # BERT: Bidirectional Encoder Representations from Transformers\n",
      "\n",
      "**BERT** is a transformer-based lan...\n",
      "\n",
      "ğŸ§  STEP 2: Relevance Assessment\n",
      "   Relevance Score: 4.0/5\n",
      "   Decision: RELEVANT\n",
      "   Reasoning: Document 1 provides substantial information on the self-attention mechanism within transformer architecture, explaining its importance in enabling parallel processing and capturing long-range dependencies, which directly addresses the user's query.\n",
      "\n",
      "âœ… STEP 3: Path A - Standard Generation\n",
      "   Confidence: HIGH - Proceeding with retrieved context\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š RESULT SUMMARY\n",
      "================================================================================\n",
      "Path Taken: A: RELEVANT\n",
      "Relevance Score: 4.0/5\n",
      "Corrective Action: None\n",
      "\n",
      "ğŸ’¡ Answer:\n",
      "The self-attention mechanism in transformer architecture is a process that allows each position in a sequence to consider and attend to all other positions within the sequence. This is achieved through the scaled dot-product attention formula, which involves computing attention scores using query, key, and value vectors. The importance of self-attention lies in its ability to capture relationships between tokens regardless of their distance in the sequence, enabling the model to understand context and dependencies more effectively. This mechanism facilitates parallel processing, enhances the model's ability to capture long-range dependencies, and contributes to the overall efficiency and performance of transformers in various tasks.\n",
      "\n",
      "ğŸ“š Sources:\n",
      "   1. transformer_architecture.md: # Transformer Architecture in Deep Learning\n",
      "\n",
      "The **Transformer architecture** revolutionized natural language processing and deep learning when introd...\n",
      "   2. transformer_architecture.md: Multi-head self-attention\n",
      "  2. Layer normalization\n",
      "  3. Feed-forward network\n",
      "  4. Residual connections around each sub-layer\n",
      "\n",
      "### Decoder Structure\n",
      "- ...\n"
     ]
    }
   ],
   "source": [
    "# Test Query 1: Expected Path A (RELEVANT)\n",
    "query1 = \"What is the self-attention mechanism in transformer architecture and why is it important?\"\n",
    "\n",
    "result1 = crag_system.query(query1, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š RESULT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Path Taken: {result1['path']}\")\n",
    "print(f\"Relevance Score: {result1['relevance_score']}/5\")\n",
    "print(f\"Corrective Action: {result1['corrective_action']}\")\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{result1['answer']}\")\n",
    "print(\"\\nğŸ“š Sources:\")\n",
    "for i, node in enumerate(result1['source_nodes'][:2], 1):\n",
    "    print(f\"   {i}. {node.node.metadata.get('file_name', 'Unknown')}: {node.node.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a27811",
   "metadata": {},
   "source": [
    "### Test Query 2: Partially Covered Topic (Expected: Path B - PARTIAL)\n",
    "Query that's tangentially related but needs better retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e154440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” QUERY: How do language models handle bidirectional context compared to unidirectional approaches?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¥ STEP 1: Initial Retrieval\n",
      "   Retrieved 3 documents\n",
      "   Doc 1: # BERT: Bidirectional Encoder Representations from Transformers\n",
      "\n",
      "**BERT** is a transformer-based lan...\n",
      "   Doc 2: Multi-head self-attention\n",
      "  2. Layer normalization\n",
      "  3. Feed-forward network\n",
      "  4. Residual connectio...\n",
      "   Doc 3: **Pre-training**: Learning from a massive text corpus to predict the next token\n",
      "2. **Instruction Tun...\n",
      "\n",
      "ğŸ§  STEP 2: Relevance Assessment\n",
      "   Relevance Score: 5.0/5\n",
      "   Decision: RELEVANT\n",
      "   Reasoning: Document 1 directly addresses the query by explaining BERT's bidirectional approach, contrasting it with unidirectional models, which is central to understanding how language models handle bidirectional context.\n",
      "\n",
      "âœ… STEP 3: Path A - Standard Generation\n",
      "   Confidence: HIGH - Proceeding with retrieved context\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š RESULT SUMMARY\n",
      "================================================================================\n",
      "Path Taken: A: RELEVANT\n",
      "Relevance Score: 5.0/5\n",
      "Corrective Action: None\n",
      "\n",
      "ğŸ’¡ Answer:\n",
      "Language models that handle bidirectional context, like BERT, process text in both directions simultaneously, allowing them to understand the context surrounding each word more comprehensively. This approach contrasts with unidirectional models, which process text either left-to-right or right-to-left, limiting their ability to capture context from both sides of a word. Bidirectional models use techniques such as masked language modeling to predict masked tokens based on the surrounding context, enhancing their ability to understand nuanced language patterns and relationships.\n",
      "\n",
      "ğŸ“š Sources:\n",
      "   1. bert_model.md: # BERT: Bidirectional Encoder Representations from Transformers\n",
      "\n",
      "**BERT** is a transformer-based language model developed by Google Research in 2018. ...\n",
      "   2. transformer_architecture.md: Multi-head self-attention\n",
      "  2. Layer normalization\n",
      "  3. Feed-forward network\n",
      "  4. Residual connections around each sub-layer\n",
      "\n",
      "### Decoder Structure\n",
      "- ...\n"
     ]
    }
   ],
   "source": [
    "# Test Query 2: Expected Path B (PARTIAL - Re-retrieval)\n",
    "query2 = \"How do language models handle bidirectional context compared to unidirectional approaches?\"\n",
    "\n",
    "result2 = crag_system.query(query2, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š RESULT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Path Taken: {result2['path']}\")\n",
    "print(f\"Relevance Score: {result2['relevance_score']}/5\")\n",
    "print(f\"Corrective Action: {result2['corrective_action']}\")\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{result2['answer']}\")\n",
    "print(\"\\nğŸ“š Sources:\")\n",
    "for i, node in enumerate(result2['source_nodes'][:2], 1):\n",
    "    print(f\"   {i}. {node.node.metadata.get('file_name', 'Unknown')}: {node.node.text[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e99629",
   "metadata": {},
   "source": [
    "### Test Query 3: Not Covered Topic (Expected: Path C - NOT RELEVANT)\n",
    "Query about a topic not in our limited corpus (Docker, recent events, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "633a993e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ” QUERY: What are the best practices for containerizing machine learning models with Docker and Kubernetes?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“¥ STEP 1: Initial Retrieval\n",
      "   Retrieved 3 documents\n",
      "   Doc 1: **Pre-training**: Learning from a massive text corpus to predict the next token\n",
      "2. **Instruction Tun...\n",
      "   Doc 2: # GPT-4: Generative Pre-trained Transformer 4\n",
      "\n",
      "**GPT-4** is OpenAI's fourth-generation large languag...\n",
      "   Doc 3: # Transformer Architecture in Deep Learning\n",
      "\n",
      "The **Transformer architecture** revolutionized natural...\n",
      "\n",
      "ğŸ§  STEP 2: Relevance Assessment\n",
      "   Relevance Score: 1.0/5\n",
      "   Decision: NOT_RELEVANT\n",
      "   Reasoning: None of the documents contain information related to containerizing machine learning models with Docker and Kubernetes.\n",
      "\n",
      "âŒ STEP 3: Path C - Fallback to Direct LLM\n",
      "   Confidence: LOW - Knowledge base lacks relevant information\n",
      "   Action: Using LLM's parametric knowledge (simulating web search)\n",
      "   âš ï¸  Note: This is a graceful fallback. In production, this would:\n",
      "      - Trigger web search (Bing, Google)\n",
      "      - Query external APIs\n",
      "      - Or return 'Information not available' with suggestions\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š RESULT SUMMARY\n",
      "================================================================================\n",
      "Path Taken: C: NOT RELEVANT (Fallback)\n",
      "Relevance Score: 1.0/5\n",
      "Corrective Action: Direct LLM generation (web search simulation)\n",
      "\n",
      "ğŸ’¡ Answer:\n",
      "Containerizing machine learning models with Docker and Kubernetes involves several best practices to ensure efficiency, scalability, and maintainability. Here are some key practices:\n",
      "\n",
      "1. **Use Lightweight Base Images**: Start with a minimal base image to reduce the size of your container. Alpine Linux is a popular choice for its small footprint.\n",
      "\n",
      "2. **Separate Dependencies**: Clearly define and separate dependencies in your Dockerfile. Use a requirements.txt file for Python dependencies and install only what's necessary.\n",
      "\n",
      "3. **Optimize Dockerfile**: Write efficient Dockerfiles by minimizing the number of layers. Combine commands where possible and use caching effectively to speed up builds.\n",
      "\n",
      "4. **Environment Variables**: Use environment variables to configure your application, making it easier to manage different configurations for development, testing, and production.\n",
      "\n",
      "5. **Model Versioning**: Implement a versioning system for your models to keep track of changes and ensure reproducibility. This can be done using tags in Docker images.\n",
      "\n",
      "6. **Security Practices**: Regularly update your base images and dependencies to patch vulnerabilities. Use Docker's security features, like user namespaces, to enhance security.\n",
      "\n",
      "7. **Resource Requests and Limits**: In Kubernetes, define resource requests and limits for your containers to ensure efficient resource utilization and avoid overconsumption.\n",
      "\n",
      "8. **Horizontal Scaling**: Design your application to be stateless and scalable. Use Kubernetes features like ReplicaSets and Horizontal Pod Autoscaler to manage scaling.\n",
      "\n",
      "9. **Monitoring and Logging**: Implement monitoring and logging solutions to track the performance and health of your containers. Tools like Prometheus and Grafana can be integrated with Kubernetes for this purpose.\n",
      "\n",
      "10. **CI/CD Integration**: Automate the build and deployment process using CI/CD pipelines. This ensures consistent and repeatable deployments.\n",
      "\n",
      "11. **Network Policies**: Use Kubernetes network policies to control the communication between pods, enhancing security and performance.\n",
      "\n",
      "12. **Persistent Storage**: If your application requires persistent storage, use Kubernetes Persistent Volumes and Persistent Volume Claims to manage storage efficiently.\n",
      "\n",
      "By following these best practices, you can effectively containerize and manage machine learning models using Docker and Kubernetes, ensuring they are scalable, secure, and maintainable.\n",
      "\n",
      "ğŸ“š Sources: 0 (fallback used LLM parametric knowledge)\n"
     ]
    }
   ],
   "source": [
    "# Test Query 3: Expected Path C (NOT RELEVANT - Fallback)\n",
    "query3 = \"What are the best practices for containerizing machine learning models with Docker and Kubernetes?\"\n",
    "\n",
    "result3 = crag_system.query(query3, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“Š RESULT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Path Taken: {result3['path']}\")\n",
    "print(f\"Relevance Score: {result3['relevance_score']}/5\")\n",
    "print(f\"Corrective Action: {result3['corrective_action']}\")\n",
    "print(f\"\\nğŸ’¡ Answer:\\n{result3['answer']}\")\n",
    "print(f\"\\nğŸ“š Sources: {len(result3['source_nodes'])} (fallback used LLM parametric knowledge)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fba6e",
   "metadata": {},
   "source": [
    "## Step 7: Comparative Analysis - Corrective vs. Baseline RAG\n",
    "\n",
    "Let's compare Corrective RAG against a baseline system that always proceeds with retrieval, regardless of quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa095e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ”¬ COMPARISON TEST\n",
      "====================================================================================================\n",
      "Query: What are the best practices for containerizing machine learning models with Docker?\n",
      "\n",
      "[1] BASELINE RAG (No Correction)\n",
      "--------------------------------------------------\n",
      "Answer: Containerizing machine learning models with Docker involves several best practices to ensure efficiency, scalability, and maintainability. Here are some key practices:\n",
      "\n",
      "1. **Use Lightweight Base Images**: Start with a minimal base image to reduce the size of the container. Alpine Linux is often recommended for its small footprint.\n",
      "\n",
      "2. **Separate Build and Runtime Environments**: Use multi-stage builds to separate the environment needed for building the model from the one needed for running it. This helps in keeping the runtime environment clean and lightweight.\n",
      "\n",
      "3. **Optimize Dependencies**: Only include necessary dependencies in the Docker image. This minimizes the image size and reduces potential security vulnerabilities.\n",
      "\n",
      "4. **Environment Variables for Configuration**: Use environment variables to configure the model's behavior, allowing flexibility without changing the container image.\n",
      "\n",
      "5. **Version Control**: Tag Docker images with version numbers to keep track of different iterations of the model and ensure reproducibility.\n",
      "\n",
      "6. **Security Best Practices**: Regularly update the base image and dependencies to patch vulnerabilities. Use non-root users within the container to enhance security.\n",
      "\n",
      "7. **Resource Management**: Set resource limits for CPU and memory usage to prevent the container from consuming excessive resources.\n",
      "\n",
      "8. **Logging and Monitoring**: Implement logging and monitoring within the container to track the model's performance and diagnose issues.\n",
      "\n",
      "9. **Testing**: Thoroughly test the containerized model in different environments to ensure it behaves as expected.\n",
      "\n",
      "10. **Documentation**: Provide clear documentation on how to build, run, and interact with the containerized model, including any necessary setup steps.\n",
      "\n",
      "By following these practices, you can create efficient and secure Docker containers for machine learning models that are easy to deploy and manage.\n",
      "\n",
      "[2] CORRECTIVE RAG (With Self-Reflection)\n",
      "--------------------------------------------------\n",
      "Path: C: NOT RELEVANT (Fallback)\n",
      "Relevance: 1.0/5\n",
      "Corrective Action: Direct LLM generation (web search simulation)\n",
      "Answer: Containerizing machine learning models with Docker is a popular approach to ensure consistency, scalability, and portability across different environments. Here are some best practices for containerizing machine learning models:\n",
      "\n",
      "1. **Use a Lightweight Base Image**: Start with a lightweight base image, such as Alpine or a minimal version of Ubuntu, to reduce the size of your Docker image. This helps in faster deployment and reduces resource consumption.\n",
      "\n",
      "2. **Separate Dependencies**: Clearly define and separate your dependencies. Use a requirements file (e.g., `requirements.txt` for Python) to list all necessary libraries and packages. This ensures that your environment is reproducible.\n",
      "\n",
      "3. **Optimize Dockerfile**: Write efficient Dockerfiles by minimizing the number of layers. Combine commands where possible and avoid installing unnecessary packages. Use multi-stage builds to separate build and runtime dependencies.\n",
      "\n",
      "4. **Environment Variables**: Use environment variables to configure your application. This allows you to change configurations without modifying the code or rebuilding the image.\n",
      "\n",
      "5. **Model Versioning**: Implement model versioning to keep track of different versions of your model. This can be done by tagging Docker images with version numbers or using a versioning system within your application.\n",
      "\n",
      "6. **Security Practices**: Follow security best practices by running containers with the least privilege necessary. Avoid running containers as root and use Docker's security features to limit access.\n",
      "\n",
      "7. **Logging and Monitoring**: Implement logging and monitoring within your containerized application to track performance and diagnose issues. Use tools like Prometheus, Grafana, or ELK stack for monitoring and logging.\n",
      "\n",
      "8. **Testing**: Test your containerized application thoroughly in different environments to ensure it behaves as expected. Use CI/CD pipelines to automate testing and deployment.\n",
      "\n",
      "9. **Resource Management**: Define resource limits for your containers to prevent them from consuming excessive resources. Use Docker's resource management features to set CPU and memory limits.\n",
      "\n",
      "10. **Documentation**: Provide clear documentation on how to build, run, and deploy your Docker containers. Include instructions for setting up the environment and troubleshooting common issues.\n",
      "\n",
      "By following these best practices, you can create efficient, secure, and scalable Docker containers for your machine learning models.\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“Š ANALYSIS:\n",
      "   âŒ Corrective RAG avoided using irrelevant context (prevented potential hallucination)\n",
      "      Baseline system may have generated answer from poor context\n"
     ]
    }
   ],
   "source": [
    "# Create baseline (non-corrective) query engine\n",
    "baseline_engine = index.as_query_engine(similarity_top_k=3)\n",
    "\n",
    "def run_comparison(query: str):\n",
    "    \"\"\"Run same query through both systems and compare\"\"\"\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"ğŸ”¬ COMPARISON TEST\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Baseline RAG (no correction)\n",
    "    print(\"[1] BASELINE RAG (No Correction)\")\n",
    "    print(\"-\" * 50)\n",
    "    baseline_response = baseline_engine.query(query)\n",
    "    print(f\"Answer: {baseline_response.response}\\n\")\n",
    "    \n",
    "    # Corrective RAG\n",
    "    print(\"[2] CORRECTIVE RAG (With Self-Reflection)\")\n",
    "    print(\"-\" * 50)\n",
    "    crag_result = crag_system.query(query, verbose=False)\n",
    "    print(f\"Path: {crag_result['path']}\")\n",
    "    print(f\"Relevance: {crag_result['relevance_score']}/5\")\n",
    "    print(f\"Corrective Action: {crag_result['corrective_action']}\")\n",
    "    print(f\"Answer: {crag_result['answer']}\\n\")\n",
    "    \n",
    "    print(\"=\"*100)\n",
    "    print(\"\\nğŸ“Š ANALYSIS:\")\n",
    "    if crag_result['decision'] == 'relevant':\n",
    "        print(\"   âœ… Both systems performed similarly (high-quality retrieval)\")\n",
    "    elif crag_result['decision'] == 'partial':\n",
    "        print(\"   âš ï¸  Corrective RAG improved retrieval via query transformation\")\n",
    "    else:\n",
    "        print(\"   âŒ Corrective RAG avoided using irrelevant context (prevented potential hallucination)\")\n",
    "        print(\"      Baseline system may have generated answer from poor context\")\n",
    "\n",
    "# Test on the Docker query (known to be out of corpus)\n",
    "run_comparison(\"What are the best practices for containerizing machine learning models with Docker?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fbc44",
   "metadata": {},
   "source": [
    "## Step 8: Visualizing the Decision Tree\n",
    "\n",
    "Let's create a summary visualization of how queries were routed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac9dea50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "ğŸ“Š CORRECTIVE RAG EXECUTION SUMMARY\n",
      "====================================================================================================\n",
      "                                  Query  Relevance Score     Decision                       Path                             Corrective Action\n",
      "         Self-attention in transformers              4.0     relevant                A: RELEVANT                             None (sufficient)\n",
      "Bidirectional vs unidirectional context              5.0     relevant                A: RELEVANT                                          None\n",
      "                Docker containerization              1.0 not_relevant C: NOT RELEVANT (Fallback) Direct LLM generation (web search simulation)\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ¯ Execution Path Distribution:\n",
      "   RELEVANT: 2 queries\n",
      "   NOT_RELEVANT: 1 queries\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compile results\n",
    "results_data = [\n",
    "    {\n",
    "        \"Query\": \"Self-attention in transformers\",\n",
    "        \"Relevance Score\": result1['relevance_score'],\n",
    "        \"Decision\": result1['decision'],\n",
    "        \"Path\": result1['path'],\n",
    "        \"Corrective Action\": result1['corrective_action'] or \"None (sufficient)\"\n",
    "    },\n",
    "    {\n",
    "        \"Query\": \"Bidirectional vs unidirectional context\",\n",
    "        \"Relevance Score\": result2['relevance_score'],\n",
    "        \"Decision\": result2['decision'],\n",
    "        \"Path\": result2['path'],\n",
    "        \"Corrective Action\": result2['corrective_action'] or \"None\"\n",
    "    },\n",
    "    {\n",
    "        \"Query\": \"Docker containerization\",\n",
    "        \"Relevance Score\": result3['relevance_score'],\n",
    "        \"Decision\": result3['decision'],\n",
    "        \"Path\": result3['path'],\n",
    "        \"Corrective Action\": result3['corrective_action'] or \"None\"\n",
    "    }\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(results_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ğŸ“Š CORRECTIVE RAG EXECUTION SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "\n",
    "# Path distribution\n",
    "path_counts = df_results['Decision'].value_counts()\n",
    "print(\"\\nğŸ¯ Execution Path Distribution:\")\n",
    "for decision, count in path_counts.items():\n",
    "    print(f\"   {decision.upper()}: {count} queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0008ef5a",
   "metadata": {},
   "source": [
    "## ğŸ“ Key Takeaways\n",
    "\n",
    "### What We Learned\n",
    "\n",
    "1. **Self-Reflection Prevents Failures**\n",
    "   - Traditional RAG blindly proceeds with retrieved context, even if irrelevant\n",
    "   - Corrective RAG evaluates quality BEFORE generation, preventing hallucinations\n",
    "   - LLM-as-judge provides flexible, semantic quality assessment\n",
    "\n",
    "2. **Adaptive Routing Improves Robustness**\n",
    "   - High relevance â†’ Standard path (efficient)\n",
    "   - Medium relevance â†’ Corrective action (query transformation, re-retrieval)\n",
    "   - Low relevance â†’ Fallback (web search, direct LLM, or \"I don't know\")\n",
    "\n",
    "3. **Failure Modes Mapping**\n",
    "   - **FP1 (Missing Content)**: Detected by Path C (not relevant) â†’ Fallback triggered\n",
    "   - **FP2 (Missed Ranking)**: Detected by Path B (partial) â†’ Re-retrieval with HyDE\n",
    "   - **FP3-FP5**: Can be caught post-generation with additional grading\n",
    "\n",
    "4. **Production Considerations**\n",
    "   - Relevance grading adds latency (extra LLM call)\n",
    "   - Can cache grading decisions for similar queries\n",
    "   - Threshold tuning (4-5, 2-4, <2) depends on domain\n",
    "   - Web search integration provides genuine fallback capability\n",
    "\n",
    "### Architectural Insights\n",
    "\n",
    "Corrective RAG represents a fundamental shift:\n",
    "- From **passive retrieval** to **active quality control**\n",
    "- From **fixed pipelines** to **adaptive workflows**\n",
    "- From **blind generation** to **self-aware systems**\n",
    "\n",
    "This is a bridge between traditional RAG and fully Agentic RAG, where the system begins to reason about its own capabilities and limitations.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To extend this implementation:\n",
    "1. **Add Web Search**: Integrate Bing/Google API for Path C\n",
    "2. **Multi-Pass Correction**: Allow multiple re-retrieval attempts\n",
    "3. **Post-Generation Validation**: Add faithfulness grading after answer generation\n",
    "4. **Confidence Calibration**: Tune thresholds based on domain-specific evaluation\n",
    "5. **Logging & Monitoring**: Track path distribution to identify corpus gaps\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š References\n",
    "\n",
    "From the curriculum (AdvancedRAGWorkshop.md):\n",
    "- **Reference 10**: Seven Failure Points When Engineering a RAG System (arXiv)\n",
    "- **Reference 11**: The Common Failure Points of LLM RAG Systems and How to Overcome Them\n",
    "\n",
    "Key concepts demonstrated:\n",
    "- Self-reflective retrieval and adaptive behavior\n",
    "- Conditional workflow based on retrieval quality assessment\n",
    "- Fallback mechanisms for graceful degradation\n",
    "- Query transformation as corrective action (HyDE)\n",
    "\n",
    "---\n",
    "\n",
    "**Status**: Demo #7 Complete âœ…"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crewai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
